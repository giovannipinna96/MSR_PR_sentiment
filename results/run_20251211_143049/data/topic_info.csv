Topic,Count,Name,Representation,Representative_Docs
-1,1281,-1_tests_think_like_code code,"['tests', 'think', 'like', 'code code', 'removed', 'use', 'case', 'does', 'commit', 'type']","[""> Pretty bummed to see a copy of this code to begin with. System.Reflection.MetadataLoadContext is a hard fork of the native AOT reflection stack. IIRC the original authors of MetadataLoadContext were planning for sharing but that never materialized so we have two copies. > Seems to me like we can add a regression-type test to either of these spots to get aot coverage, but I'm not seeing a great place where we have a shared codebase for all reflection tests. @MichalStrehovsky do you have a suggestion? System.Reflection.Tests you mentioned above would work, we do run those on native AOT. We don't currently run any of the MetadataLoadContext tests due to the way they are written (#72908 - they will not work on any single file form factor, native AOT included; these tests are also problematic on mobile form factors for the same reason)."", ""The [CODE] method was completely removed in Android Billing Client v8.0.0. After analyzing the generated API, only [CODE] and [CODE] methods remain. I've updated the method to return a proper error response with [CODE] and marked it as obsolete with migration guidance to use [CODE] instead. Fixed in commit 1e67483."", 'I ran [CODE] at the package root, but encountered issues with the dependencies required to run the full [CODE] command. The process requires building multiple TypeSpec packages including the compiler, which is failing due to missing dependencies. Since the documentation structure with the customization guide in [CODE] and the link to it in both the readme.md and usage.md files looks correct, the PR should fulfill the requirements. The issue with running regen-docs appears to be related to the environment setup rather than our documentation changes.']"
0,2096,0_remove_delete_copilot remove_copilot,"['remove', 'delete', 'copilot remove', 'copilot', 'file', 'comment', 'line', 'pr', 'remove file', 'changes']","[""It appears that ghalint's ability to ignore lint is incomplete and not all rules can be excluded.Also, the rules that could be excluded were not necessary because they were not covered in our implementation. ref: https://github.com/suzuki-shunsuke/ghalint/tree/main?tab=readme-ov-file#disable-policies Therefore, we implemented the exclusion process on the job side as a workaround. The results can be seen here: - Cases where action_ref_should_be_full_length_commit_sha fails: https://github.com/liam-hq/liam/actions/runs/15870057039/job/44744378667?pr=2198 - Other cases where lint fails: https://github.com/liam-hq/liam/actions/runs/15870111155/job/44744534855?pr=2198"", 'Actually this should just be a simple check to block on _all_ errors, not just these errors. No need to be extremely conservative here. I think all of these block execution in the end. This may indeed break existing tests. But the best thing to do is probably to make the minimal changes to get those tests to pass. Why were those tests failing? Did the test Q# have compiler errors? That was probably not intended - can you fix them?', '@mitchdenny This error message is incorrect when you set overrideMinimumSdkVersion. <img width=""2574"" height=""804"" alt=""image"" src=""https://github.com/user-attachments/assets/591d2e7b-2531-438c-80e5-45efa16bd0d3"" />']"
1,34,1_duration_timeout_delays_calculation,"['duration', 'timeout', 'delays', 'calculation', 'backoff', 'performance', 'timing', 'cancelled', 'perf', 'limit']","['agree. anyway CAT seems to be very broken at the moment. Under very light load, it works occasionally, but once I increase the number of transactions, the chain halts. I am right now going over the whole implementation to familiarize with it and see what can be improved. So, I am fine with merging this to be on the same version as v0.38.x branch. Otherwise, we could revert that commit and wait until we fix the real issues', '[nitpick] The exponential backoff delays may significantly extend test runtime under repeated failures. You could cap the total backoff time or make the delays configurable to prevent long CI runs. [CODE_BLOCK]', '[nitpick] The timeout value of 1100ms (1.1 seconds) is longer than the original 1 second timeout, which seems counterintuitive for fixing a timing issue. Consider using a shorter duration like 900ms or add a comment explaining why a longer timeout is needed to fix the interruption test. [CODE_BLOCK]']"
2,26,2_azure_bicep_nuget_www,"['azure', 'bicep', 'nuget', 'www', 'https www', '13', 'sdk', 'ps1', 'packages', '15']","[""@Copilot not only did you completely forget to make the necessary adjustment in [CODE] (which are admittedly a little finicky, as you have to use [CODE] to retrieve the [CODE] setting, but then also match that URL against the Azure Repo patterns). No, you also failed to introduce a new test case ([CODE]) to verify that change. Also, you test for [CODE] here, but that's absolutely not what we want to set! This would apply to all remote URLs, not only to the Azure Repos ones. You _definitely_ need to initialize a new repository (either via [CODE] or via [CODE]) and then configure a valid-looking Azure Repo URL in there, then call [CODE] and _then_ verify that the [CODE] is set (substituting the actual URL for [CODE]). Back to work!"", ""Azure.Identity 1.13.1 is deprecated https://www.nuget.org/packages/Azure.Identity/1.13.1 Note that 1.15.0-beta1 https://www.nuget.org/packages/Azure.Identity/1.15.0-beta.1#dependencies-body-tab removes the unnecessary reference to System.Memory on net8.0 unknown when that will GA but based on history that's in 2-3 months unless you can speed that up."", ""Azure.Identity 1.13.1 is deprecated https://www.nuget.org/packages/Azure.Identity/1.13.1 Note that 1.15.0-beta1 https://www.nuget.org/packages/Azure.Identity/1.15.0-beta.1#dependencies-body-tab removes the unnecessary reference to System.Memory on net8.0 unknown when that will GA but based on history that's in 2-3 months unless you can speed that up.""]"
3,18,3_async_store_follows code_block_retrieve,"['async', 'store', 'follows code_block', 'retrieve', 'follows', 'state', 'sample', 'settings', 'way', 'needs']","['async_update_mapping is not async in store. make it async there.', 'async_delete_module is not async in store. make it async.', 'async_delete_mapping is not an async method. make it async in store.']"
4,16,4_sdfg_program_architecture_dace,"['sdfg', 'program', 'architecture', 'dace', 'target code', 'tizen', 'native', 'code generator', 'symbols', 'interesting']","[""I see how this is different than CoreCLR reflection, but right now I'm not seeing a small enough change without side effects to bring them closer. Considering all accessors would introduce allocation and also would not gain anything without further changes to MLC - it's also not fixing any reported issue. I do see that we could realized longer term benefit by sharing more of the managed code in CoreCLR reflection and MLC - but right now they are built on different metadata models - so fixing this is not in the scope of this PR. I would be interesting in how we could do that longer term though."", 'The manually created SDFG does not have the same functionality as the DaCe-Python program you originally wrote. I believe the program was incorrect because you tried to define the symbol inside it with [CODE]. However, this is incorrect. Symbols (defined or not) must be defined outside of the program. For example, as follows: [CODE_BLOCK] Maybe give it a try and see if it works?', 'This is not a good example. It does not showcase anything other than the target code generation, which is already easy to modify in the current structure (see [CODE] and [CODE]). The example needs to show something new and more involved. Please replace this with an OpenCL code generator that modifies, e.g., the allocation process and copies. You can then use the same example to demonstrate how an SDFG file might be split to another SDFG for the kernel portion alone (i.e., for the .cl file) and how the target code generation pass interacts with this.']"
5,16,5_task_tasks_achieve_unreliable,"['task', 'tasks', 'achieve', 'unreliable', 'code_block using', 'run code', 'incorrect error', 'iterator', 'ids', 'reporting']","['This fallback warning suggests the task tracking logic may be unreliable. The warning message should include more debugging information like the total number of tasks and tracked tools to help diagnose the issue. [CODE_BLOCK]', 'Using [CODE] as task_id in the error handling is incorrect because [CODE] returns the number of remaining tasks, not the ID of the failed task. This will likely map to the wrong tools in the error case. [CODE_BLOCK]', ""Using [CODE] as task_id in error handling is incorrect. When a task fails, [CODE] represents the number of remaining tasks, not the failed task's ID. This will cause incorrect error attribution. [CODE_BLOCK]""]"
6,13,6_timeout anymore_need timeout_dont think_timeout,"['timeout anymore', 'need timeout', 'dont think', 'timeout', 'anymore', 'think need', 'dont', 'fallback logic', 'v2', 'apphost']","['i dont think you need this timeout anymore', 'i dont think you need this timeout anymore', 'i dont think you need this timeout anymore']"
7,11,7_garbage_pages_memory_collection,"['garbage', 'pages', 'memory', 'collection', 'lifetime', 'wants', 'release', 'management', 'applications', 'advanced']","["">, but the RCW doesn't automatically release the underlying COM object when it's garbage collected. This is invalid statement. RCW does release the underlying COM object when it's garbage collected. The problem is the delayed nature of garbage collection. not that the RCW is not released."", ""Doesn't the not implemented error sound random? Glb -> Glb being more explicitly specified in a code means that it needs to be copied within a map (e.g. calling copytomap as a transformation)? As if there is a program that absolutely needs glb to glb memory copy (or the user wants that), I think asking for different memory locations to be used is irrelevant. Different memory scopes wont help and we don't have explicit synchronization patterns in dace (also what will adding a syncthreads will do)?"", 'I do not think that this pattern should be followed by most production applications out there. The delayed release of the COM objects that is a side-effect of garbage collection is not a problem for most production applications. Vast majority of developers love that .NET garbage collection takes care of it for them and they are perfectly fine paying a small price for it. Stepping back, this is introductory sample for Office Interop. it is appropriate to complicate it by manual lifetime management. I think it may be better to leave this sample alone, and move the advanced topic of manual lifetime management to a new advanced sample.']"
8,10,8_rust_converted_probably doesn_need checks,"['rust', 'converted', 'probably doesn', 'need checks', 'err', 'code probably', 'attempted', 'stream', 'macro', 'avoiding']","[""This code probably doesn't need any checks does it? Won't the Rust code return an Err if undefined is attempted to be converted into an Object or Function? If so, just remove all of these changes."", 'I don\'t like this as a solution, because it changes the contract for this helper to something a lot less intuitive: > The resultant token stream will be an expression which typechecks at the Rust type ... becomes > The resultant token stream will typecheck at the Rust type of the unnamed type of the result, or the empty stream if named results are used which means that it can\'t generally be used to uniformly generate a return value. Right now, this expression is only used in the tail position of a function definition, so it works ""by coincidence""---but this change makes the interface more brittle (e.g. if you wanted to refactor the macro to trace the return value, you would need special cases to not accidentally generate an empty [CODE] statement).', ""This is not avoiding checking our code, it's avoiding checking the types of libraries in use. It is common to do so unless your app is a critical production app, which is not the case here. Type checking the whole node-modules is overengineering, for a product that will simply lag then restart if one of the type in the three was used wrong. If your app is a production app and a crash will lead to loses of 1M$, you should double-check every package you use. The Rust code should be validated profoundly, as it can crash the user's entire system, and while not horrible, it is a big bug for a tool. But the dependencies used for the VS Code plugin can have a regular check, and a deep check is an overkill. In case we choose to do check libs, the results will not be upgrading to 24, as it will take time until all libs are aligned with the new rules, and we lose the performance edge. So, as you see right. My position is to skip lib check. @cordx56 please give your opinion as well on the matter. Edit: You don't need to listen to the gentleman below, I didn't ask for his opinion. I'm also not sure if I'm excited to see that copilot repeat what is said, or that it means that I'm going to be replaced ;p""]"
9,10,9_lock_number_don touch_don change,"['lock', 'number', 'don touch', 'don change', 'test don', 'touch', 'passing', 'passed', 'need run', 'supported']","[""Don't touch yarn.lock."", 'When hitSlop is the same in all directions it can be passed with just a number giving the same behavior as passing an object with all the directions as same number. please fix this.', 'When hitSlop is the same in all directions it can be passed with just a number giving the same behavior as passing an object with all the directions as same number. please fix this.']"
10,10,10_backend_code context_consistently_cloned,"['backend', 'code context', 'consistently', 'cloned', 'particular setup', 'redis', 'way need', 'env var', 'version doesn', 'previous version']","[""ah yeh, i guess it wasnt smart enough to set up an env var [CODE]...but that's also a context engineering issue"", 'This subset will never successfully build without very particular setup. Is it worth having here? This would effectively make the ""all subsets"" option only usable for restore.', ""So interestingly, this doesn't seem to work with the latest version of redis...but also, the previous version doesn't support saving things in the way we now need to now we aren't passing the URL in to the model, so we have to use this! Thanks for pointing it out.""]"
11,9,11_generic_port_generic documentation_original,"['generic', 'port', 'generic documentation', 'original', 'output port', 'retained', 'valuable', 'key source', 'documentation generic', 'source output']","[""The documentation is too generic. The original comment 'The key of the source output port.' was more descriptive and should be retained or improved rather than replaced with a generic getter/setter description. [CODE_BLOCK]"", ""[nitpick] Generic documentation 'Represents a TieLineCollection' is less informative than the original. The original documentation provided context about it being a collection of TieLine objects, which was more descriptive. [CODE_BLOCK]"", ""The generic documentation 'Gets or sets the SourcePort' provides no meaningful information beyond the property name. The original comment 'The key of the source output port' was more descriptive and should be retained or enhanced. [CODE_BLOCK]""]"
12,9,12_fees_manager_balance_contract,"['fees', 'manager', 'balance', 'contract', '1m', 'adjustment', 'misconfiguration', 'dropped', 'going add', 'code accordingly']","[""well, the flip side of that is if the designated fee manager keeper is down, then the fees won't get adjusted. You really want both replicas to be symmetric. Also, the fee adjustment runs on a 30 second polling interval, so you're fairly unlikely to get a confilct."", 'also I just realized that you need the fee manager wallet for [CODE] as well. only the fee manager can set the current fees :( this is actually going to add some complexity to the fee withdrawal logic, as we also need to make sure the fee_manager wallet has a sufficient balance.', ""btw this min_balance minimum withdrawal has actually caused outages in the past. If for whatever reason, the accrued fees aren't > min_balance (often this is a misconfiguration of the chain), the keeper wallet runs out of funds even though there's money to withdraw in accrued fees. let me suggest changing this to [CODE]. I.e., you can withdraw a tiny amount from the contract, as long as you're at least doubling the balance in your wallet. That should naturally limit the number of withdrawals in the case where the accrued fees are below the min_balance""]"
13,9,13_flag_bank_flags_behaviors,"['flag', 'bank', 'flags', 'behaviors', 'setting flag', 'price', 'symmetric', 'code gets', 'endpoints', 'example code']","['Actually this looks suspicious. Why are we setting this flag here?', '(aside) @tejasbadadare in the future you probably want to get rid of both of these flags. You want the keepers to be symmetric so there should be no reason to disable any of these behaviors.', 'Yep, this is annoying but (1) somebody put the emissions flags on the ""main"" bank struct so we can\'t move those, (2) the bank doesn\'t get passed to all the price/risk functions, for example [CODE] gets just the config. It seemed easier to add flags to the config than to refactor these to take a bank.']"
14,9,14_studio figma_tokens studio_tokens_plugin start,"['studio figma', 'tokens studio', 'tokens', 'plugin start', 'figma plugin', 'figma', 'studio', 'plugin', 'start', 'assignable']","['Reverted the wrong changes to append-nodes.ts.test.cjs snapshot file back to the original state before my changes. Commit b23c0a2.', ""My problems are as follows: - I'm using Angular 19 (so it's incompatible with your modification) - I can't override npm i (due to my company's restrictions) - With the previous version ng-extract-i18n-merge@^2.14.3, I have this checkmarx vulnerability: ![image](https://github.com/user-attachments/assets/506a704d-23d4-425d-85f0-493ea061a47f) But if its not possible to keep angular 19, I'll stick with your version 2.14.3 and ignore the vulnerability."", ""there is an error here, please fix it: ERROR in ./src/app/components/StartScreen.tsx:133:41 @tokens-studio/figma-plugin:start: TS2345: Argument of type 'URLStorageType | JSONBinStorageType | GitHubStorageType | GitLabStorageType | GenericVersionedStorageType | BitbucketStorageType | ADOStorageType | SupernovaStorageType | TokensStudioStorageType' is not assignable to parameter of type 'StorageTypeCredentials'. @tokens-studio/figma-plugin:start: Type 'URLStorageType' is not assignable to type 'StorageTypeCredentials'. @tokens-studio/figma-plugin:start: Type 'URLStorageType' is not assignable to type 'StorageTypeCredential<URLStorageType, true>'. @tokens-studio/figma-plugin:start: Property 'secret' is missing in type 'URLStorageType' but required in type '{ secret: string; }'. @tokens-studio/figma-plugin:start: 131 | onClick: () => { @tokens-studio/figma-plugin:start: 132 | const matchingProvider = apiProviders.find((i) => i.internalId === storageType?.internalId); @tokens-studio/figma-plugin:start: > 133 | restoreStoredProvider(matchingProvider || storageType); @tokens-studio/figma-plugin:start: | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ @tokens-studio/figma-plugin:start: 134 | }, @tokens-studio/figma-plugin:start: 135 | text: t('retry'), @tokens-studio/figma-plugin:start: 136 | }} @tokens-studio/figma-plugin:start: @tokens-studio/figma-plugin:start: Found 1 error in 2449 ms. @tokens-studio/figma-plugin:start: ERROR in ./src/app/components/StartScreen.tsx:133:41 @tokens-studio/figma-plugin:start: TS2345: Argument of type 'URLStorageType | JSONBinStorageType | GitHubStorageType | GitLabStorageType | GenericVersionedStorageType | BitbucketStorageType | ADOStorageType | SupernovaStorageType | TokensStudioStorageType' is not assignable to parameter of type 'StorageTypeCredentials'. @tokens-studio/figma-plugin:start: Type 'URLStorageType' is not assignable to type 'StorageTypeCredentials'. @tokens-studio/figma-plugin:start: Type 'URLStorageType' is not assignable to type 'StorageTypeCredential<URLStorageType, true>'. @tokens-studio/figma-plugin:start: Property 'secret' is missing in type 'URLStorageType' but required in type '{ secret: string; }'. @tokens-studio/figma-plugin:start: 131 | onClick: () => { @tokens-studio/figma-plugin:start: 132 | const matchingProvider = apiProviders.find((i) => i.internalId === storageType?.internalId); @tokens-studio/figma-plugin:start: > 133 | restoreStoredProvider(matchingProvider || storageType); @tokens-studio/figma-plugin:start: | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ @tokens-studio/figma-plugin:start: 134 | }, @tokens-studio/figma-plugin:start: 135 | text: t('retry'), @tokens-studio/figma-plugin:start: 136 | }} @tokens-studio/figma-plugin:start: @tokens-studio/figma-plugin:start: Found 1 error in 4227 ms.""]"
15,9,15_exports_exported_download_export,"['exports', 'exported', 'download', 'export', 'members', 'looks redundant', 'need able', 'guides', 'api longer', 'correctly https']","['I\'m having a difficult time groking the ""production_from_snapshot"" item. I guess the idea here is that we now: - support restore from snapshots (is this - if you restore your production branch, you\'ll get a ""productions_from_snapshot"" branch I\'d suggest leaving this one out for now.', ""I don't think we want download in this list -- project owners and staff members (not just admins) need to be able to download bulk exports."", 'https://nextjs.org/docs/app/guides/static-exports#unsupported-features headers are not supported in export mode.']"
16,9,16_model factory_factory_destroy_factory method,"['model factory', 'factory', 'destroy', 'factory method', 'instead add', 'exclude', 'azd', 'model', 'method don', 'operation']","[""Don't exclude violations of AZC0035. Instead, add the model factory method"", ""Don't exclude violations of AZC0035. Instead, add the model factory method"", ""@Copilot The changes did not address the concerns raised by @vbhb1989 . The goroutine is launched separately, program will continue and exit, resulting in the spawwned goroutine here ending prematurely. This results in bugs of either: the destroy operation never runs, the destroy operation runs partially, the destroy operation does not invalidate the environment keys. With how azd is designed currently, I'm not sure if there is a straightforward implementation here that isn't already solved by existing Bash and Powershell backgrounding techniques. The cli currently needs to waits for the destroy operation to succeed to invalidate the env keys.""]"
17,8,17_docker_package json_deployment_abstract,"['docker', 'package json', 'deployment', 'abstract', 'directly need', 'build process', 'volume', 'sure makes', 'mitchdenny', 'note cause']","['This part is incorrect. kubectl run does not support --volumes directly. You need to define a volume using a pod or deployment spec instead.', '@mitchdenny this is docker specific. Podman doesn’t have a daemon. I’m not sure it makes sense to abstract this method. PS I’m working on a functional test', 'The browserslist section has been removed from docker-extension/ui/package.json as requested. ab60ea1 Note: This may cause build issues since react-scripts requires browser targets to be specified. The build process will prompt to add defaults if needed.']"
18,8,18_pattern_dont_file dont_patterns,"['pattern', 'dont', 'file dont', 'patterns', 'change', 'file', '', '', '', '']","['dont change this pattern', 'dont change this pattern', 'dont change this pattern']"
19,8,19_enabled_throw error_automatically_enable,"['enabled', 'throw error', 'automatically', 'enable', 'throw', 'attempting use', 'want enable', 'enabling', 'publish', 'code throw']","[""We don't want it to automatically enable streams if not already enabled. Please edit the codebase to instead throw an error if you try to enable replication and streams aren't already enabled. You should be able to check the streamOptions property to determine that."", ""I've updated the code to throw an error if streams aren't enabled when attempting to use replication instead of automatically enabling them. This change is in commit 8ac5e1e."", ""I've modified the code to throw an error if streams aren't already enabled when attempting to use replication instead of automatically enabling them. The error message clearly indicates that streams must be enabled for Global Tables replication. This change is in commit 8ac5e1e.""]"
20,8,20_i18n_groups_route_language,"['i18n', 'groups', 'route', 'language', 'issue check', 'languages', 'organize', 'messed', 'gupta', 'translated']","['Check on this also. The language makes it sounds like everybody will need a full refresh', '@gupta-piyush19 The LLM messed up the translations here. In German, the second sentence should read [CODE] The other languages seem to have deteriorated as well (at least the French translation shows exactly the same issue).', 'the text is still wrong ### Advanced i18n with Route Groups Organize i18n routes using route groups for better structure: there are no route groups being used']"
21,7,21_aspire_supported_constraint_non existent,"['aspire', 'supported', 'constraint', 'non existent', 'existent', 'clarify', 'table', 'restart', 'apphost', 'change pretty']","[""@copilot you should revert this change - I'm pretty sure it's a product-level constraint for Aspire that they need 9.0.3xx. I was just noting the impact this constraint has on the user base."", 'Fixed the reference to clarify that resource limits aren\'t supported in .NET Aspire. Updated the table to show ""Not supported"" instead of the non-existent [CODE] method. 7c7651f', 'Fixed the reference to clarify that restart policies aren\'t supported in .NET Aspire. Updated the table to show ""Not supported"" instead of the non-existent [CODE] method. 81a03e3']"
22,7,22_add link_alternative approach_allow option_globally,"['add link', 'alternative approach', 'allow option', 'globally', 'externally', 'need don', 'tell ai', 'rate limited', 'pr https', 'just turn']","[""I wouldn't consider this really a next step, it's more like an alternative approach. I'd consider removing this link here."", ""DevinAI don't need this now that this PR is https://github.com/calcom/cal.com/pull/21940 merged. getRoutedUrl is already rate limited"", ""This section is mostly to tell the AI, which can't follow links (well the OpenAI one can't, the Google one can, but it's worse at its job). I can add the link in addition though.""]"
23,6,23_date_code_block stop_stop using_hey copilot,"['date', 'code_block stop', 'stop using', 'hey copilot', 'error lines', 'like use', 'hey', 'confused', 'code set', 'supports']","['stop using Date.now() in the key ... we need these to be deterministic', 'Use [CODE] and set an error along the the lines of ""Failed to parse \'{v}\' as a timestamp"".', 'Hey @copilot, you seem to get confused with what the date is... seems like you use January 8th a lot. Just use the current date instead. [CODE_BLOCK]']"
24,6,24_dictionary_thread_modifies_safety,"['dictionary', 'thread', 'modifies', 'safety', 'safer', 'throws', 'pretty', 'possibly', 'thrown', 'contract']","['Same here - This code change modifies the behavior in a possibly breaking manner. Dictionary.Add throws an ArgumentException when the key already exists. ConcurrentDictionary.TryAdd will silently ignore that fact. I think it would be safer to maintaint that API contract - so, if TryAdd retruns false it should thrown an ArgumentException simulating what Dictionary.Add is doing.', ""I initially tried using SyncRoot but found that for generic collections like List&lt;T&gt;, SyncRoot doesn't provide thread safety - it just returns the list itself. The documentation recommends using dedicated synchronization objects for thread-safe operations. I've reverted to using a lock object to ensure proper thread safety. Would you prefer a different approach?"", 'One way why I did not like the initial implementation using locks is that read-access would also need to be covered under a lock - not sure whether single thread writing -+ multiple readers would also break that bad in Dictionary<TKey, TValue> - but since the documentation leaves that pretty open and we ran into pretty bad issues with normal Dictionary in the past, using ConcurrentDictionary is pretty much always the right call in our SDK. The theoretic overhead is so minimal that the benefit of getting this safe outweights it']"
25,6,25_github actions_actions_dex_product,"['github actions', 'actions', 'dex', 'product', 'container', 'github', 'yaml', 'needs copied', 'mechanism code_block', 'file needs']","['The github actions workflow for test-linux now fails due to timeout. Please double check why and if this test case might be the problem', ""That product page url does not seem to exist and is redirecting to a GitHub Actions product page. Let's point to the docs instead: [CODE_BLOCK]"", ""This volume mount will not work in GitHub Actions because the relative path './frontend/cypress/dex.yaml' refers to the GitHub Actions runner's file system, not the container. The dex.yaml file needs to be copied into the container or made available through a different mechanism. [CODE_BLOCK]""]"
26,5,26_nodes_react_consistent cross_js code,"['nodes', 'react', 'consistent cross', 'js code', 'partially', 'js', 'enter', 'code_block isn', 'tooltip', 'cross']","[""This still isn't working, you forgot to include the react types. look at other tests for an example"", '[nitpick] The onKeyPress event is deprecated in React; prefer using onKeyDown for consistent cross-browser behavior when detecting Enter key presses. [CODE_BLOCK]', 'Alright, what you have is partially working, but still seems to error on some reparsed nodes. For example,, this test case shows that we are spuriously erroring on this JS code, even though the nodes with types are reparsed. There must be some places in the current code that are missing reparsed checks.']"
27,5,27_middleware_route_code_block code_block_health,"['middleware', 'route', 'code_block code_block', 'health', 'globally', 'slow', 'maybe better', 'returned code', 'code urls', 'better use']","['This should be inside [CODE] so that urls that did not have a result is not handled by this middleware.', ""this example is very much not correct, it returns the middleware, instead of an ASGI app, and even if it returned [CODE], that guy hasn't been transformed, maybe it would be better to use litestar or something similar as an example?"", ""would like to separate route and middleware logic that didn't work well with the health middleware back then and caused countless bug reports where the route is defined, the timeout middleware can be defined with a specific time or globally you can specify a route in app.use and also operate handler chaining [CODE_BLOCK] or [CODE_BLOCK] or [CODE_BLOCK]""]"
28,5,28_bookworm_bookworm tree_com foxminchan_foxminchan bookworm,"['bookworm', 'bookworm tree', 'com foxminchan', 'foxminchan bookworm', 'foxminchan', 'tree main', 'tree', 'src services', 'refer https', 'services copilot']","['@copilot wrong service, please refer https://github.com/foxminchan/BookWorm/tree/main/src', '@copilot this is not correct, please refer in https://github.com/foxminchan/BookWorm/tree/main/src/Services', '@copilot this is incorrect, please refer: https://github.com/foxminchan/BookWorm/tree/main/src/Services']"
