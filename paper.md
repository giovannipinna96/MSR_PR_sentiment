# Sentiment Analysis of Code Review Friction in AI-Assisted Pull Requests

## Abstract

The integration of AI coding agents such as GitHub Copilot, Devin, Claude Code, and OpenAI Codex into software development workflows has transformed how code is generated and reviewed. However, little is known about the friction these AI-generated contributions create during code review processes. This study presents a comprehensive sentiment analysis of 20,635 code review items (13,613 inline comments and 7,022 top-level reviews) from pull requests generated by five distinct AI agents, sourced from the AIDev dataset and filtered to include only repositories with 100+ GitHub stars. Using a RoBERTa-based transformer model for sentiment classification and BERTopic for topic modeling, we investigate four research questions concerning sentiment patterns, agent-specific friction differences, friction-generating topics, and correlations with PR outcomes. Our results reveal statistically significant differences in friction levels across AI agents (Kruskal-Wallis H=1011.49, p<0.001), with OpenAI Codex generating the lowest friction (M=0.120) and Copilot the highest (M=0.252). We also find that inline code review comments exhibit significantly higher friction (22.7% negative) than top-level reviews (13.0% negative). Topic analysis identifies code style issues as the dominant friction category (55.0%), followed by security concerns (18.2%) and testing-related feedback (17.1%). We find significant correlations between friction and both time-to-merge (r=0.123, p<0.001) and review iterations (r=0.389, p<0.001). These findings provide actionable insights for AI agent developers and software engineering teams adopting AI-assisted development practices.

## 1. Introduction

### 1.1 Background

The emergence of AI-powered coding assistants represents a paradigm shift in software development. Tools such as GitHub Copilot, Anthropic's Claude Code, Cognition's Devin, and OpenAI Codex have demonstrated remarkable capabilities in generating code, fixing bugs, and implementing features autonomously. As these agents become increasingly integrated into development workflows, understanding their impact on collaborative software engineering processes becomes crucial.

Code review serves as a critical quality gate in software development, where human reviewers examine proposed changes before integration. The interaction between AI-generated code and human reviewers creates a unique dynamic that may differ fundamentally from traditional human-to-human code review interactions. Understanding the "friction" in these interactions—defined as negative sentiment expressed during review—provides valuable insights into the challenges and opportunities of AI-assisted development.

### 1.2 Research Questions

This study addresses four research questions:

- **RQ1**: How does review comment sentiment manifest for Agentic-PRs?
- **RQ2**: Are there friction differences across different AI agents?
- **RQ3**: Which specific topics (security, testing, code style, logic) generate the most friction?
- **RQ4**: How do friction metrics correlate with outcomes (rejection, iterations, time-to-merge)?

### 1.3 Contributions

This paper makes the following contributions:

1. A large-scale empirical analysis of sentiment in AI-generated pull request reviews across five major AI coding agents
2. A comparative analysis of friction patterns between inline code review comments and top-level reviews
3. Identification of friction patterns across different PR types (features, fixes, documentation, etc.)
4. Statistical evidence of significant differences in friction levels across AI agents
5. Correlation analysis between friction metrics and PR lifecycle outcomes

## 2. Methods

### 2.1 Dataset

We utilize the AIDev dataset from HuggingFace (hao-li/AIDev), a comprehensive collection of AI agent-generated pull requests on GitHub. The dataset encompasses pull requests from multiple AI agents including Copilot, Devin, Claude Code, OpenAI Codex, and Cursor.

**Dataset Selection:**

We use the `pull_request` subset of the AIDev dataset, which contains only pull requests from repositories with 100 or more GitHub stars. This pre-filtered subset (as opposed to `all_pull_request` which contains all PRs regardless of repository popularity) ensures analysis of high-quality, established projects. This criterion serves multiple purposes:
- Ensures projects have established code review practices
- Filters out experimental or personal repositories
- Focuses on projects with active maintainer communities

**Data Quality Filtering:**

We apply temporal filtering to exclude PRs that were closed in less than 1 minute after creation, as these typically represent:
- Automated test PRs
- Spam or accidental submissions
- Bot-generated noise

This filtering removed 10,210 PRs from the original 33,596, leaving 23,386 PRs for analysis.

**Data Sources:**

Unlike previous analyses that focused solely on inline code comments, this study analyzes both:
1. **Inline review comments** (`pr_review_comments_v2`): Line-specific feedback attached to code changes
2. **Top-level reviews** (`pr_reviews`): General review feedback not tied to specific lines

**Dataset Statistics:**

| Metric | Value |
|--------|-------|
| Total AI PRs (100+ stars repos) | 33,596 |
| PRs after temporal filter (≥1 min duration) | 23,386 |
| PRs removed (closed <1 min) | 10,210 |
| Inline Review Comments | 13,613 |
| Top-Level Reviews | 7,022 |
| **Total Items Analyzed** | **20,635** |
| AI Agents Represented | 5 |

**Agent Distribution:**

| AI Agent | Total Items | Percentage |
|----------|-------------|------------|
| Copilot | 9,845 | 47.7% |
| Devin | 5,057 | 24.5% |
| OpenAI_Codex | 3,618 | 17.5% |
| Cursor | 1,464 | 7.1% |
| Claude_Code | 651 | 3.2% |

**PR Type Distribution:**

Pull requests are classified by type based on conventional commit prefixes and PR title analysis:

| PR Type | Count | Percentage |
|---------|-------|------------|
| feat (new features) | 9,244 | 44.8% |
| fix (bug fixes) | 4,932 | 23.9% |
| docs (documentation) | 2,559 | 12.4% |
| refactor | 1,714 | 8.3% |
| chore | 702 | 3.4% |
| test | 681 | 3.3% |
| build | 289 | 1.4% |
| ci | 186 | 0.9% |
| perf | 148 | 0.7% |
| style | 89 | 0.4% |
| revert | 52 | 0.3% |
| other | 39 | 0.2% |

### 2.2 Sentiment Analysis

**Model Selection:**

We employ the `cardiffnlp/twitter-roberta-base-sentiment-latest` model for sentiment classification. While this model was originally trained on Twitter data, recent research (2024) demonstrates that transformer-based models consistently outperform domain-specific sentiment analysis tools for software engineering contexts, including SentiCR, Senti4SD, and SentiStrength-SE, by up to 35.6% in F1 scores.

**Friction Score Definition:**

For each review comment or review, we define the friction score as the probability of negative sentiment:

```
Friction Score = P(negative | text)
```

This continuous measure (0-1) captures the intensity of negative sentiment rather than a binary classification, enabling more nuanced statistical analysis.

**Processing:**

- Batch processing with batch_size=32
- Text truncation at 512 tokens (BERT maximum)
- GPU-accelerated inference when available
- Separate analysis for comments and reviews to enable comparative insights

### 2.3 Topic Modeling

We employ BERTopic for unsupervised topic discovery on negative comments, following established best practices:

**Configuration:**

- Embedding model: `all-MiniLM-L6-v2`
- Vectorizer: CountVectorizer with English stopwords, min_df=2, ngram_range=(1,2)
- ClassTfidfTransformer with reduce_frequent_words=True
- Minimum topic size: 5 comments

This configuration ensures meaningful topic representations by:
1. Preserving full context during embedding generation (no preprocessing)
2. Removing stopwords only in topic representation phase
3. Using n-grams to capture multi-word concepts

**Category Classification:**

In addition to unsupervised topic modeling, we employ zero-shot classification using `facebook/bart-large-mnli` to categorize negative comments into predefined friction categories:
- Testing
- Security
- Code Style
- Logic
- Documentation

### 2.4 Statistical Analysis

We employ the following statistical methods:

1. **Kruskal-Wallis H-test**: Non-parametric test for comparing friction distributions across multiple AI agents
2. **Post-hoc Mann-Whitney U tests**: Pairwise comparisons when Kruskal-Wallis is significant
3. **Chi-square test**: Testing independence between friction categories and AI agents
4. **Point-biserial correlation**: Assessing relationship between friction and merge outcomes
5. **Spearman correlation**: Evaluating relationships between friction and time-to-merge/iterations

## 3. Results

### 3.1 RQ1: Sentiment Distribution for Agentic-PRs

Analysis of 20,635 review items reveals distinct sentiment patterns between inline comments and top-level reviews:

**Overall Sentiment Distribution:**

| Source Type | Negative | Neutral | Positive | Mean Friction |
|-------------|----------|---------|----------|---------------|
| Inline Comments | 22.7% | 62.1% | 15.2% | 0.253 |
| Top-Level Reviews | 13.0% | 74.6% | 12.4% | 0.155 |
| **Combined** | **19.4%** | **66.2%** | **14.4%** | **0.220** |

**Key Finding:** Inline code review comments exhibit significantly higher friction than top-level reviews. This suggests that line-specific feedback tends to be more critical, while top-level reviews often contain more neutral or approving summaries.

**Mean Friction Scores by Agent:**

| AI Agent | Mean Friction | Std Dev | n |
|----------|---------------|---------|---|
| OpenAI_Codex | 0.120 | 0.192 | 3,618 |
| Claude_Code | 0.183 | 0.232 | 651 |
| Cursor | 0.231 | 0.260 | 1,464 |
| Devin | 0.231 | 0.263 | 5,057 |
| Copilot | 0.252 | 0.278 | 9,845 |

### 3.2 RQ2: Friction Differences Across AI Agents

**Kruskal-Wallis Test:**

The Kruskal-Wallis H-test reveals highly significant differences in friction distributions across AI agents:

- H-statistic: 1011.49
- p-value: 1.16 × 10⁻²¹⁷
- **Result: Highly significant (p < 0.001)**

**Key Finding:** OpenAI Codex exhibits significantly lower friction than all other agents, with a mean friction score of 0.120 compared to Copilot's 0.252. This represents a substantial difference in how reviewers respond to code generated by different AI agents.

**Friction by Agent and Source Type:**

| AI Agent | Comments (Mean) | Reviews (Mean) | Difference |
|----------|-----------------|----------------|------------|
| Claude_Code | 0.223 | 0.129 | 0.094 |
| Copilot | 0.267 | 0.191 | 0.076 |
| Cursor | 0.232 | 0.228 | 0.004 |
| Devin | 0.244 | 0.198 | 0.046 |
| OpenAI_Codex | 0.205 | 0.078 | 0.127 |

**Key Finding:** OpenAI Codex shows the largest gap between comment and review friction, suggesting that while inline comments may be critical, top-level reviews are notably more positive for this agent.

**Temporal Evolution:**

Analysis of friction trends over time (January 2025 - July 2025) reveals:
- **Devin**: Shows increasing friction over time (r=0.204, p<0.001), rising from ~0.09 in March to ~0.25 in July
- **Claude_Code**: Exhibits increasing friction (r=0.231, p<0.001), from ~0.07 in June to ~0.16 in July
- **OpenAI_Codex**: Maintains consistently low friction (~0.06-0.09) throughout the period (r=0.141, p<0.001)
- **Copilot**: Shows relatively stable friction around 0.18-0.20
- **Cursor**: Maintains stable friction around 0.22-0.23

### 3.3 RQ3: Topic Analysis

**Category Distribution in Negative Comments:**

| Category | Count | Percentage | Mean Friction |
|----------|-------|------------|---------------|
| Code Style | 2,199 | 55.0% | 0.691 |
| Security | 726 | 18.2% | 0.694 |
| Testing | 685 | 17.1% | 0.704 |
| Logic | 253 | 6.3% | 0.672 |
| Documentation | 135 | 3.4% | 0.658 |

**Statistical Significance:**

- Kruskal-Wallis (categories): H=22.08, p=0.0002 - **Significant**
- Chi-square (category × agent): χ²=82.13, p<0.001 - **Highly significant**

**Category Distribution by Agent:**

| Category | Claude_Code | Copilot | Cursor | Devin | OpenAI_Codex |
|----------|-------------|---------|--------|-------|--------------|
| Code Style | 54 | 1,181 | 202 | 602 | 160 |
| Security | 20 | 449 | 39 | 177 | 41 |
| Testing | 11 | 450 | 34 | 134 | 56 |
| Logic | 10 | 110 | 28 | 74 | 31 |
| Documentation | 4 | 63 | 14 | 39 | 15 |

**Key Finding:** Testing-related friction has the highest mean friction score (0.704), indicating that when reviewers identify testing issues, they express stronger negative sentiment. Code style, while most common, has relatively lower intensity per comment.

**BERTopic Results:**

Unsupervised topic modeling identified 137 distinct topics. Top friction topics include:

1. **Topic 0** (code_block, code code_block): General code structure issues - dominant across all agents
2. **Topic 1** (copilot remove, remove line): Agent-specific removal requests - heavily weighted toward Copilot
3. **Topic 2** (gensx, invoices, packages): Domain-specific concerns - prominent in Cursor and Devin
4. **Topic 3** (remove test, test remove): Test cleanup requests
5. **Topic 15** (benchmark, op, alert): Performance and alerting issues - unique to OpenAI Codex

### 3.4 RQ4: Correlation with Outcomes

**Time-to-Merge Analysis:**

- Spearman correlation: r = 0.123
- p-value: 2.10 × 10⁻¹⁴
- **Result: Significant positive correlation**

Higher friction scores are associated with longer time-to-merge, with a moderate effect size.

**Review Iterations:**

- Spearman correlation: r = 0.389
- p-value: 2.48 × 10⁻²¹⁷
- **Result: Highly significant positive correlation**

PRs with higher friction experience substantially more review iterations (measured as comment count per PR). This is the strongest correlation observed.

**Temporal Correlation by Agent:**

| Agent | Correlation (r) | p-value | Significant |
|-------|-----------------|---------|-------------|
| Claude_Code | 0.231 | 0.0001 | Yes |
| Devin | 0.204 | <0.0001 | Yes |
| OpenAI_Codex | 0.141 | <0.0001 | Yes |
| Copilot | 0.033 | 0.148 | No |
| Cursor | 0.050 | 0.131 | No |

### 3.5 PR Type Analysis (New)

**Friction by PR Type:**

| PR Type | Mean Friction | Std Dev | Observations |
|---------|---------------|---------|--------------|
| style | 0.478 | 0.311 | Highest friction |
| revert | 0.432 | 0.276 | High friction |
| ci | 0.288 | 0.284 | Above average |
| chore | 0.265 | 0.272 | Above average |
| test | 0.227 | 0.263 | Average |
| fix | 0.223 | 0.260 | Average |
| feat | 0.221 | 0.261 | Average |
| refactor | 0.217 | 0.253 | Average |
| build | 0.207 | 0.261 | Below average |
| docs | 0.127 | 0.197 | Low friction |
| perf | 0.125 | 0.211 | Low friction |

**Key Finding:** Style-related PRs generate the highest friction (M=0.478), nearly double the average. This aligns with the finding that code style is the dominant friction category. Documentation and performance PRs receive the most positive reception.

**PR Type × Agent Interaction:**

Notable patterns from the heatmap analysis:
- **Style PRs + Copilot**: Extremely high friction (0.666)
- **Style PRs + Cursor**: Very high friction (0.521)
- **Revert PRs + Devin**: High friction (0.435)
- **Docs PRs + OpenAI_Codex**: Very low friction (0.077)

## 4. Figures

### 4.1 Core Friction Analysis Figures

**Figure 1: Friction Score by AI Agent (Boxplot)**
*File: `friction_boxplot.png`*

This boxplot displays the distribution of friction scores across five AI agents. **Observable trends:** OpenAI Codex shows a notably compressed distribution with the lowest median (approximately 0.04) and smallest interquartile range, indicating consistently low-friction interactions. Copilot, Cursor, and Devin exhibit similar distributions with medians around 0.08-0.10 and extensive whiskers reaching to 0.95, reflecting high variability. Claude Code shows a moderate distribution with numerous high-friction outliers above 0.6. **Key insight:** The dramatic difference between OpenAI Codex and other agents is visually striking, with its upper quartile falling below the median of most other agents.

**Figure 2: Friction Score Distribution (Violin Plot)**
*File: `friction_violin.png`*

The violin plot reveals the full density distribution of friction scores. **Observable trends:** All agents show bimodal or heavily right-skewed distributions, with a dominant mode near 0 (low friction) and a secondary mode around 0.7-0.9 (high friction). OpenAI Codex has the most concentrated mass near zero with a thin tail. Copilot and Devin show broader "bodies" indicating more uniform distribution of friction across the range. **Key insight:** The shape reveals that most reviews are low-friction, but when friction occurs, it tends to be substantial (>0.6).

**Figure 3: Friction Distribution Histogram**
*File: `friction_distribution_histogram.png`*

Stacked histogram showing frequency of friction scores by agent. **Observable trends:** The distribution is heavily right-skewed with the majority of all observations (>60%) falling in the 0-0.1 range. OpenAI Codex dominates the lowest friction bin. Copilot shows a distinctive secondary peak in higher friction bins (0.7-0.9), contributing disproportionately to high-friction interactions. **Key insight:** The long right tail demonstrates that while most interactions are positive, high-friction events are not rare.

**Figure 4: Sentiment Distribution Across AI Agents**
*File: `sentiment_distribution.png`*

Grouped bar chart showing counts of negative, neutral, and positive sentiments by agent. **Observable trends:** Neutral sentiment dominates across all agents (60-75% of items). Copilot has the highest absolute count of negative sentiment due to its larger sample size, but also maintains the highest neutral count. OpenAI Codex shows the most balanced positive-to-negative ratio. **Key insight:** The relative proportions remain fairly consistent across agents, with the neutral category always dominant.

### 4.2 Category and Topic Figures

**Figure 5: Friction Categories Distribution (Pie Chart)**
*File: `category_distribution_pie.png`*

Pie chart showing the distribution of friction categories among negative comments. **Observable trends:** Code Style dominates at 55.0%, representing over half of all friction-generating comments. Security (18.2%) and Testing (17.1%) form the second tier. Logic (6.3%) and Documentation (3.4%) are minor contributors. **Key insight:** More than half of all friction relates to stylistic concerns rather than functional issues, suggesting AI agents struggle more with conventions than logic.

**Figure 6: Friction Score by Category (Boxplot)**
*File: `category_friction_boxplot.png`*

Boxplot comparing friction intensity across categories. **Observable trends:** Testing has the highest median friction (approximately 0.70) and the widest interquartile range. Security and Code Style show similar medians around 0.69. Documentation has the lowest median (0.65) and smallest range. All categories show friction scores concentrated in the 0.55-0.85 range with few outliers. **Key insight:** When friction occurs, its intensity is relatively consistent across categories, though Testing issues tend to generate slightly stronger negative sentiment.

**Figure 7: Category-Agent Heatmap**
*File: `category_agent_heatmap.png`*

Heatmap showing the count of friction instances by category and agent. **Observable trends:** Copilot dominates all categories due to sample size, with Code Style (1,181) being particularly prominent. Devin shows elevated Security concerns (177) relative to its sample size. Claude Code has minimal representation across all categories. **Key insight:** The distribution pattern suggests Copilot's high volume may be driving overall trends, and normalized analysis may reveal different patterns.

**Figure 8: Topic-Agent Interaction Matrix**
*File: `topic_agent_heatmap.png`*

Large heatmap showing 137 topics across 5 agents. **Observable trends:** Topic 0 (general code issues) is the dominant topic across all agents. Topic 1 (Copilot-specific removal requests) shows strong concentration in Copilot column. Several topics appear agent-specific (e.g., Topic 15 concentrated in OpenAI Codex, Topic 19 exclusive to Devin). **Key insight:** While some topics are universal, others reflect agent-specific code patterns that generate unique friction signatures.

### 4.3 Source Comparison Figures

**Figure 9: Friction Score - Comments vs Reviews**
*File: `friction_by_source.png`*

Boxplot comparing friction between inline comments and top-level reviews. **Observable trends:** Comments show a higher median (approximately 0.11 vs 0.04) and substantially larger interquartile range. Reviews have a more compressed distribution with many outliers above the whiskers. **Key insight:** Inline comments are inherently more critical than summary reviews, likely because they address specific code issues.

**Figure 10: Mean Friction by Agent and Source**
*File: `friction_agent_by_source.png`*

Grouped bar chart showing mean friction by agent, split by source type. **Observable trends:** All agents show higher friction in comments than reviews, but the gap varies. OpenAI Codex has the largest comment-review gap (0.205 vs 0.078). Cursor shows nearly identical friction for both sources (0.232 vs 0.228). **Key insight:** Different agents receive qualitatively different feedback types, with some agents receiving more consistent treatment across feedback venues.

**Figure 11: Sentiment by Source**
*File: `sentiment_by_source.png`*

Comparison of sentiment distributions between comments and reviews. **Observable trends:** Reviews show a higher proportion of neutral sentiment. Comments have nearly double the negative sentiment rate. Positive sentiment is slightly higher in comments. **Key insight:** Top-level reviews serve a different communicative function, often summarizing rather than critiquing.

### 4.4 PR Type Figures

**Figure 12: Friction Score by PR Type (Boxplot)**
*File: `by_pr_type/friction_by_pr_type.png`*

Boxplot showing friction distribution across 12 PR types. **Observable trends:** Style PRs show dramatically higher friction with median around 0.48 and extensive high-friction observations. Revert PRs follow with median around 0.43. Docs and Perf PRs show the lowest friction with compressed distributions near 0.04. Feature and Fix PRs, despite being most common, show moderate median friction around 0.08-0.09. **Key insight:** PR type strongly predicts expected friction level, with style changes being controversial and documentation changes being well-received.

**Figure 13: PR Type Distribution (Pie Chart)**
*File: `by_pr_type/pr_type_distribution.png`*

Pie chart showing the distribution of PR types. **Observable trends:** Feature PRs dominate at 44.8%, followed by Fix (23.9%) and Docs (12.4%). Specialized types (CI, Style, Revert, Perf) collectively represent <5% of PRs. **Key insight:** AI agents are primarily used for feature development and bug fixing, with relatively limited use for infrastructure or style changes.

**Figure 14: PR Type × Agent Friction Heatmap**
*File: `by_pr_type/friction_heatmap_type_agent.png`*

Heatmap showing mean friction for each PR type-agent combination. **Observable trends:**
- **Hotspots (high friction):** Style+Copilot (0.666), Style+Cursor (0.521), Revert+Devin (0.435)
- **Cool spots (low friction):** Build+Claude_Code (0.013), Docs+OpenAI_Codex (0.077), Test+OpenAI_Codex (0.093)
- Several cells are empty (white), indicating no PRs of that type from that agent
**Key insight:** The interaction effects are substantial—the same PR type can generate very different friction depending on the agent.

**Figure 15: Negative Rate by PR Type**
*File: `by_pr_type/negative_rate_by_pr_type.png`*

Bar chart showing percentage of negative sentiment by PR type. **Observable trends:** Style PRs have the highest negative rate (~45%), followed by Revert (~40%) and CI (~30%). Docs and Perf PRs have the lowest negative rates (~10-12%). **Key insight:** The negative rate closely mirrors mean friction scores, confirming consistency in measurement approaches.

### 4.5 Outcome Correlation Figures

**Figure 16: Friction vs Time-to-Merge**
*File: `friction_vs_timemerge.png`*

Scatter plot with friction on x-axis and time-to-merge (hours) on y-axis. **Observable trends:** Dense clustering near origin (low friction, fast merge) with sparse points extending to high friction and long merge times. No single agent dominates the high-friction/long-merge quadrant. Maximum merge times exceed 1750 hours (~73 days). **Key insight:** While correlation exists (r=0.123), the relationship is not deterministic—many high-friction PRs still merge quickly, and some low-friction PRs take extended time.

**Figure 17: Friction vs Review Iterations**
*File: `friction_vs_iterations.png`*

Scatter plot with friction on x-axis and number of review comments on y-axis. **Observable trends:** Stronger positive relationship visible compared to time-to-merge plot. High-iteration PRs (>30 comments) tend to cluster in the 0.15-0.35 friction range rather than at extremes. Copilot (purple) dominates high-iteration PRs. **Key insight:** The stronger correlation (r=0.389) indicates friction more directly predicts review effort than merge time.

**Figure 18: Temporal Evolution of Friction**
*File: `temporal_evolution.png`*

Line chart showing mean friction by month for each agent. **Observable trends:**
- Devin shows U-shaped pattern: declining early 2025, then sharply increasing to ~0.25 by July
- OpenAI Codex maintains remarkably stable low friction (~0.06-0.09) throughout
- Copilot emerges in May 2025 with increasing trend
- Cursor appears in June 2025, relatively stable
- Claude Code shows volatility, rising from 0.07 to 0.16

**Key insight:** Agent friction profiles are not static—some improve over time while others degrade, possibly reflecting model updates, changing use cases, or evolving reviewer expectations.

### 4.6 Figure Summary

The visualizations collectively reveal several macro-level patterns in AI-assisted code review friction:

**1. Agent Stratification:** A clear hierarchy exists among AI agents in terms of friction generation. OpenAI Codex consistently produces the lowest friction across all analyses (boxplots, histograms, heatmaps), while Copilot generates the highest mean friction. This stratification is statistically significant (H=1011.49, p<10⁻²¹⁷) and practically meaningful, with a 2× difference between lowest and highest agents.

**2. Source Type Matters:** Inline comments exhibit systematically higher friction than top-level reviews across all agents. This pattern is consistent and substantial (22.7% vs 13.0% negative rate), suggesting that the context of feedback delivery shapes its sentiment characteristics. Reviewers appear more critical when addressing specific code lines.

**3. PR Type Predicts Friction:** The type of change being reviewed strongly predicts friction level. Style and Revert PRs generate 3-4× more friction than Documentation and Performance PRs. This suggests AI agents should be deployed strategically—they excel at documentation and performance optimization but struggle with style-sensitive changes.

**4. Category Concentration:** Over half (55%) of all friction relates to Code Style issues, indicating a systematic weakness in AI agents' ability to adhere to project-specific conventions. Security and Testing concerns form the secondary cluster, suggesting these remain areas requiring human oversight.

**5. Temporal Dynamics:** Friction is not static over time. Some agents (Devin) show increasing friction trends that may warrant investigation, while others (OpenAI Codex) maintain consistent low friction. This suggests ongoing model improvements or degradation that development teams should monitor.

**6. Outcome Correlations:** Both time-to-merge and review iterations correlate positively with friction, but the iteration correlation (r=0.389) is substantially stronger than the time correlation (r=0.123). This indicates friction primarily affects review effort rather than calendar time, possibly because high-friction PRs require more back-and-forth but maintainers still prioritize them.

**7. Interaction Effects:** The heatmaps reveal important interaction effects. The same agent can generate very different friction depending on PR type, and the same PR type generates different friction depending on the agent. For example, Style PRs from Copilot generate extreme friction (0.666) while Style PRs from Devin generate moderate friction (0.254).

These patterns provide actionable guidance for teams adopting AI coding assistants: choose agents appropriate to the task type, expect higher friction for style-related changes, and monitor friction trends over time as both agents and codebases evolve.

## 5. Discussion

### 5.1 Key Findings

**OpenAI Codex's Lower Friction:** OpenAI Codex demonstrates significantly lower friction levels compared to other AI agents. This may be attributed to:
- Training on curated code samples with consistent style
- Integration with environments that enforce style pre-submission
- Selective deployment in projects with clearer conventions

**Copilot's Higher Friction Despite Market Dominance:** GitHub Copilot, despite having the largest market presence, shows the highest mean friction. This paradox may reflect:
- Broader deployment across diverse project types
- Use in contexts where other agents aren't attempted
- More aggressive code generation that deviates from project conventions

**Code Style as Primary Friction Source:** The preponderance of code style issues (55.0%) suggests AI agents may struggle with project-specific conventions, formatting requirements, and idiomatic patterns. This represents a key opportunity for improvement in AI agent training and customization.

**Comments vs Reviews Distinction:** The significant difference in friction between inline comments (22.7% negative) and top-level reviews (13.0% negative) reveals that feedback venue matters. This suggests:
- Line-level feedback is inherently more critical
- Top-level reviews serve a summarizing function
- Analyzing only one feedback type provides incomplete picture

**Strong Iteration Correlation:** The r=0.389 correlation between friction and review iterations is notably stronger than typical effect sizes in software engineering research. This indicates friction is a meaningful predictor of review effort and should be monitored.

### 5.2 Implications

**For AI Agent Developers:**
1. Prioritize code style consistency and project convention adherence
2. Enhance security-aware code generation
3. Consider implementing pre-submission linting and validation
4. Focus on documentation and performance tasks where agents excel
5. Provide customization options for project-specific conventions

**For Development Teams:**
1. Establish clear style guides for AI agent configuration
2. Consider agent-specific review checklists based on identified friction patterns
3. Monitor friction trends as agents receive updates
4. Deploy different agents for different PR types based on friction profiles
5. Analyze both inline comments and top-level reviews for complete picture

### 5.3 Limitations

1. **Temporal scope:** Analysis limited to January-July 2025 data
2. **Sentiment model domain:** While transformer models outperform SE-specific tools, the model was not fine-tuned on code review text
3. **Causal inference:** Correlations do not establish causality between friction and outcomes
4. **Sample size imbalance:** Copilot dominates the dataset, potentially skewing aggregate patterns
5. **PR type classification:** Relies on conventional commit parsing which may misclassify some PRs

## 6. Threats to Validity

### Internal Validity
- Sentiment classification accuracy limited by model capabilities
- Category classification relies on zero-shot inference
- Temporal filtering (1-minute threshold) may exclude some legitimate quick-close PRs

### External Validity
- Results may not generalize to private repositories or different technology stacks
- 100+ stars filter may introduce selection bias toward successful projects
- Focus on English-language comments limits generalizability

### Construct Validity
- Friction score as proxy for review difficulty
- Comment count as proxy for review iterations
- PR type classification based on title/commit conventions

## 7. Conclusions

This study presents a comprehensive analysis of code review friction in AI-assisted pull requests, analyzing 20,635 items (13,613 inline comments and 7,022 top-level reviews) across five major AI coding agents. Our findings reveal:

1. **Significant inter-agent differences**: AI agents exhibit statistically significant differences in friction generation, with OpenAI Codex producing consistently lower friction (M=0.120) than Copilot (M=0.252).

2. **Source type matters**: Inline code review comments generate significantly higher friction (22.7% negative) than top-level reviews (13.0% negative), highlighting the importance of analyzing both feedback types.

3. **Code style as primary friction source**: Over half (55.0%) of negative feedback relates to code style issues, suggesting a key area for AI agent improvement.

4. **PR type predicts friction**: Style and Revert PRs generate dramatically higher friction than Documentation and Performance PRs, indicating strategic deployment opportunities.

5. **Strong outcome correlations**: Higher friction is associated with longer merge times (r=0.123) and substantially more review iterations (r=0.389).

6. **Temporal dynamics**: Friction levels evolve over time differently for each agent, with some improving and others degrading.

These findings contribute to our understanding of AI-human collaboration in software development and provide actionable guidance for both AI agent developers and software engineering teams. As AI coding assistants continue to evolve, monitoring and minimizing friction in code reviews will be essential for their successful integration into development workflows.

## References

1. AIDev Dataset. HuggingFace. https://huggingface.co/datasets/hao-li/AIDev
2. Cardiff NLP Twitter RoBERTa Sentiment. https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest
3. BERTopic: Neural topic modeling. https://maartengr.github.io/BERTopic/
4. Zhang et al. (2020). "Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?" ICSME 2020.

---

*Analysis conducted using the FrictionAI pipeline. Results generated on 2025-12-13.*
